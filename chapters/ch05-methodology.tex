\chapter{Experiment Methodology}
\label{ch05:methodology}
To address the research question \textbf{"How does the performance of a Kafka Connect-based replication pipeline for Adabas on mainframe compare to the Adabas Event Replicator Target Adapter?"}, an experiment was conducted to test the performance of both systems. As the hypothesis states that the Kafka-based solution will outperform \ac{ART} only when its performance capabilities are leveraged, the experiment was designed in a way to take into account different parallelization configurations of the Kafka replication pipeline.

\section{Experiment Design}
\label{ch05:methodology:design}
The experiment involved generating a large volume of create or update changes in Adabas to test the throughput of both systems over a period of time. Due to \ac{ART} being run on one machine, while the replication pipeline is distributed, two throughput metrics were chosen that provide comparable results. These are the number of \textbf{transactions per second}, as well as \textbf{messages per second}. For the scope of this experiment, only the performance of Kafka-related components that processed the replication data was measured. This included the source and sink connector workers, as well as the Kafka brokers. The schema registry's performance was not measured, as it was used as an additional service and did not affect the replication data's throughput.

\subsection{Defining the Metrics}
In the case of Kafka replication, \textit{messages} refer to the number of total Kafka events transferred in the pipeline. As explained in \ref{ch04:pipelinedevelopment:implementation:transformingtorelational}, due to each Adabas record being normalized and split into smaller records in the source connector, there is more message traffic in the replication pipeline than just the single Adabas record. This made it difficult to compare it to the way that \ac{ART} works, as \ac{ART} processes the \ac{REPTOR}'s entire XML message that it receives and transforms the records internally before writing them to the target database. To have a comparable metric of messages per second, it was instead tracked by how many \textit{table rows}, which are identical to the split records in Kafka, were created by \ac{ART} and written to the target database.

The transaction throughput metric was more straightforward, as both solutions process the same total number of transactions. However, as mentioned in \ref{ch04:pipelinedevelopment:solutionlimitations}, transactionality is only supported by the source connector. Therefore, the transaction throughput was only measured for the source connector and compared with the \ac{ART} throughput for an approximate comparison.

\subsection{Measuring the Metrics}
The message and transaction throughput of the Kafka source connector and \ac{ART}, as well as the message throughput of the sink connector, were measured with Zipkin. In the case of the Kafka connectors, there is already built-in Zipkin support using so-called Kafka interceptors\footnote{\url{https://github.com/openzipkin-contrib/brave-kafka-interceptor}} to capture the Zipkin traces. For \ac{ART} and for transaction tracing, custom Zipkin traces were implemented using the Brave\footnote{\url{https://github.com/openzipkin/brave}} library. To measure the message throughput of the Kafka brokers, \ac{JMX} metrics were used. Unfortunately, the existing \ac{JMX} metrics did not support measuring the transaction throughput of the brokers. Therefore, the performance metrics were evaluated without taking into account the broker transaction throughput. If the broker(s) become the bottleneck, this can still be identified by monitoring the message throughput.

% The experiment is operating under the assumption that the Kafka brokers will not be the performance bottleneck in the replication pipeline if the brokers are configured in a way to optimize parallelization. This can be done by increasing the number of partitions in the Kafka topic, allowing throughput to be increased through a higher degree of parallelization \cite{cerezo2021analysisparallelism}. Therefore, the performance metrics will be evaluated without taking into account the broker transaction throughput.
% can look at byte throughput of entire kafka system (not comparable to art though) to see which system is the bottleneck?

The experiment was also intentionally designed to measure the throughput of components up until the data reached the \ac{JDBC} driver. This allows for the disregard for any latencies that result from the upsert operations to the target database. The target relational database used for the experiment was Postgres as a result of existing familiarity with it.
% maybe include that want to isolate performance and not include database variability - however, database can still be a bottleneck

\subsection{Initial State Data}
Due to the upsert nature of the \ac{JDBC} sink connector (see \ref{ch04:pipelinedevelopment:parallelizationconsiderations}), there is no real difference between the insert and update operations for the replication pipeline, therefore their performance does not need to be tested separately. Since the latencies with respect to the target database operations and any delete operations are outside of the scope, the \ac{IS} operations were used to simulate a high-throughput situation. This made it easier to prepare the Adabas file beforehand, as the data had to be inserted only once, and there was no need to delete or re-insert records in Adabas for every experiment run. This helped not only with saving time between each run, but also with reproducibility, as the number of records, record content, and generated \ac{REPTOR} message structures remained the same for every test run. The only difference in the messages resulted from \ac{ART} requiring the XML format, while the source connector required the binary format.

\subsection{Kafka Replication Scenarios}
\label{ch05:methodology:design:scenarios}
There were different Kafka pipeline setups tested against the \ac{ART} solution, to see whether or not parallelization would play a decisive factor in out-performing \ac{ART}, and to see which pipeline setup will ensure maximum performance. Each scenario was run and measured five times to minimize possible variability. The same was done for measuring the performance of \ac{ART}. The scenarios were as follows:
\begin{description}
    \item [One Task, One Partition]
    First, the performance of the replication pipeline was tested without any parallelism involved: the source and sink connectors were run on one Connect worker each, with only one task each. There was a single Kafka broker with only one partition.

    \item[Three Tasks on One Worker, Three Partitions]
    This scenario tested the parallelization capabilities of Kafka. There were three source tasks and three sink tasks. Two workers were used in total, one for the sink connector and one for the source connector. There were also three brokers, each broker being assigned partition leader for a single partition.

    \item[Seven Tasks on One Worker, Seven Partitions]
    This scenario tested the parallelization capabilities of Kafka to a higher degree than in the previous scenario. There were seven source tasks and seven sink tasks, run on one worker each. Seven partitions were configured, balanced among three brokers. The broker number was kept at three to test whether the brokers could handle a higher partition number\footnote{According to a Confluent blog post, a single Kafka broker can handle multiple thousand partitions and still maintain high availability (\url{https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/}).}.

    \item[Seven Tasks on Two Workers, Seven Partitions]
    This scenario was similar to the previous one. In this case, however, the seven source tasks and seven sink tasks were balanced on two Connect workers for the source and sink connector each (with four workers in total). The aim was to provide an insight into whether the distribution of tasks would provide additional throughput improvements.
    
    \item[20 Tasks on Five Workers, 20 Partitions] The last scenario used a significant increase in parallelization. It was selected to see whether the performance would scale effectively. The source and sink connectors were run on 5 workers each, with 20 tasks assigned per connector. The 20 partitions were hosted on three brokers.
    
\end{description}

The different scenarios can be compared to various Kafka Connect set-up types. The first scenario is often used for initial development without parallelization, while the second scenario represents a typical non-production test environment. The last three scenarios can be considered as variations of production environments, depending on workload and performance requirements. % Typically, configuration based on performance necessities and data volume - here that is not the case, as want to portray different scenarios -> CITE!!

In production environments, it is always recommended to enable partition replication on multiple brokers for additional fault tolerance. In the case of this experiment, no replication was enabled, to minimize its effect on Kafka's performance \cite{dobbelaerekafkavsrabbitmq}.

The number of tasks per connector and topic partitions was kept the same in order to take full advantage of the parallelization that partitions offer. An odd number of brokers were used for the KRaft leadership elections to work properly \cite{kraftconfluentdocumentation}.

% talk about # of tasks per partition (or vice versa) - tasks partition wise for jdbc connector?

% what configurations of kafka will be tested?

% \section{Pipeline Configuration}
% \label{ch05:methodology:pipelineconfiguration}
% \newpage
\section{Adabas File}
\label{ch05:methodology:adabasfile}
One Adabas file was used for testing the replication performance. It is called \textit{EMPLOYEES}, and contains sample employee data. This file was chosen because it had all of the existing field types described in \ref{ch02:fundamentals:adabas:forzos:datastructures}. It contained 171405 total records. Below is the \ac{FDT} showing all of the fields that will be replicated to the target database:
\begin{verbatim}
Field Description Table: File 1     (EMPLOYEES)                              
========================                Total Fields without SDT ... 31      
*************** T o p  of  F D T ***************                             
Lev  I Name I Leng  I Form  I    Options       I Predict Fld Name or DT / SY 
-----I------I-------I-------I----------------- I-----------------------------
  1  I  AA  I  008  I    A  I  DE UQ           I   PERSONNEL_ID              
  1  I  AB  I       I       I                  I   FULLNAME                  
  2  I  AC  I  020  I    A  I  NU              I   FIRST_NAME                
  2  I  AE  I  020  I    A  I  DE              I   NAME                      
  2  I  AD  I  020  I    A  I  NU              I   MIDDLE_NAME               
  1  I  AF  I  001  I    A  I  FI              I   MARSTAT                   
  1  I  AG  I  001  I    A  I  FI              I   SEX                       
  1  I  AH  I  004  I    P  I  DE NC           I   BIRTH                     
  1  I  A1  I       I       I                  I   FULL_ADDRESS              
  2  I  AI  I  020  I    A  I  MU NU           I   ADDRESS_LINE              
  2  I  AJ  I  020  I    A  I  DE NU           I   CITY                      
  2  I  AK  I  010  I    A  I  NU              I   POSTCODE                  
  2  I  AL  I  003  I    A  I  NU              I   COUNTRY                   
  1  I  A2  I       I       I                  I   TELEPHONE                 
  2  I  AN  I  006  I    A  I  NU              I   AREACODE                  
  2  I  AM  I  015  I    A  I  NU              I   PHONE                     
  1  I  AO  I  006  I    A  I  DE              I   DEPT                      
  1  I  AP  I  025  I    A  I  DE NU           I   JOBTITLE                  
  1  I  AQ  I       I       I  PE              I   INCOME                    
  2  I  AR  I  003  I    A  I  NU              I   CURRCODE                  
  2  I  AS  I  005  I    P  I  NU              I   SALARY                    
  2  I  AT  I  005  I    P  I  MU NU           I   BONUS                     
  1  I  A3  I       I       I                  I   LEAVE_DATA                
  2  I  AU  I  002  I    U  I                  I   LEAVE_DUE
  2  I  AV  I  002  I    U  I  NU              I   LEAVE_TAKEN               
  1  I  AW  I       I       I  PE              I   LEAVE_BOOKED              
  2  I  AX  I  008  I    U  I  NU              I   LEAVE_START               
  2  I  AY  I  008  I    U  I  NU              I   LEAVE_END                 
  1  I  AZ  I  003  I    A  I  DE MU NU        I   LANG                      
  1  I  BA  I  014  I    U  I  NU CR           I   INSERT                    
  1  I  BB  I  014  I    U  I  MU NU           I   UPDATE                    
\end{verbatim}

The \textit{Name} field refers to the Adabas file shortnames, which are used internally to identify the fields. Each shortname can have a longname mapped to it (shown in the \ac{FDT} as \textit{Predict Fld Name}). \textit{Form} shows the format of the fields: \textbf{A}lphanumeric, \textbf{P}acked decimals, and \textbf{U}npacked decimals. \textit{Lev} indicates the nested level of a field: for instance, ADDRESS\_LINE is a multiple field that is nested in the FULL\_ADDRESS group. Meanwhile, the \textit{Options} field describes field attributes. The relevant options include MU and PE, which indicate whether the field is a multiple field or a periodic group.

\section{Environment Setup with Azure}
\label{ch05:methodology:environmentsetup}
The experiment was chosen to run on the Azure Cloud platform to simulate a production environment. Each Kafka broker, Connect worker and \ac{ART} instance was run on a separate \ac{VM} provisioned in Azure, while Adabas and \ac{REPTOR} were run on a Software GmbH mainframe. Azure was chosen because of an existing corporate subscription and existing infrastructure. This included an existing connection to the Software GmbH intranet for access to Adabas, as well as an existing subnet and virtual network.

\subsection{Ensuring Reproducibility}
Multiple measures were taken to ensure the reproducibility of the results and to control as many variables as possible to minimize variability. Terraform was chosen as an \ac{IaC} tool to allow the provisioning of a testing infrastructure, allowing it to be reproduced at will. This allowed more control over the provisioning of \ac{VM}s, as well as the management of the testing environment.

The Ubuntu Server distribution was chosen as the \ac{OS} for all \ac{VM}s due to its open source nature. All applications were containerized using Docker, allowing ease of setup and ensuring consistent and isolated environments for each experiment run. Using Docker simplified the task of stopping and starting new instances of all applications between experiment repetitions, ensuring that previous measurements and actions did not affect the performance of subsequent runs.

To control for the effect of hardware on performance, the \ac{VM}s used for hosting \ac{ART} and the Kafka pipeline components were all provisioned with the same specifications. The Azure D-series \ac{VM} was used, as it is optimized for general-purpose production workloads\footnote{\url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}}. Table \ref{tab:hardwarespecs} lists the important hardware specifications. The Postgres database was run on a machine with identical hardware.


\begin{table}
    \centering
    \begin{tabular}{|cc|}
        \hline
         \textbf{Operating System} & Ubuntu Server 22.04 LTS \\
         \textbf{Processor} & AMD EPYC 7763 64-Core \\
         \textbf{Memory} & 16 GiB \\
         \textbf{vCPUs} & 4 \\
        \hline
    \end{tabular}
    \caption{Hardware Specifications for Performance Tests}
    \label{tab:hardwarespecs}
\end{table}

% - using terraform

% - multiple experiment runs for each scenario (take average?)

% - restarting VMs for every experiment run
% use of terraform, restarting every time with fresh kafka brokers and connectors - only metrics collection is kept running?


\subsection{Kafka-based Solution}
For the Kafka Connect pipeline, each Connect worker and Kafka broker were run on a separate \ac{VM} to simulate nodes in an actual cluster. An additional \ac{VM} with the same specifications was provisioned as the "control \ac{VM}" to host the schema registry and a web UI of the Kafka cluster. Even when running a single worker for the first experiment scenario, the worker was still run in distributed mode\footnote{The "Exactly Once" semantic in the source connector is only available when running in distributed mode (see \ref{ch04:pipelinedevelopment:solutionlimitations}).}. Listing \ref{lst:terraform} shows an example Terraform script used to provision a source connect worker.

\subsection{Target Adapter}
As \ac{ART} does not support parallelization, it was run as a single \ac{VM} instance with the same specifications shown in Table \ref{tab:hardwarespecs}. The message and transaction throughput was tracked using Zipkin and Elasticsearch.

\subsection{Metrics Collection}
The metrics collection was performed using Grafana and Prometheus for the \ac{JMX} metrics, and Zipkin, Elasticsearch, and Kibana for the message and transaction tracing. Due to the high \ac{CPU} and memory requirements for Zipkin and Elasticsearch, all metrics collection was hosted on an on-premise machine with a high amount of available memory. All components were also started as Docker containers. The Elasticsearch cluster was configured with three nodes to handle the high volume in message traces that had to be collected.