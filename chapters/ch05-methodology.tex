\chapter{Experiment Methodology}
\label{ch05:methodology}
To address the research question \textbf{"How does the performance of a Kafka Connect-based replication pipeline for Adabas on mainframe compare to the Adabas Event Replicator Target Adapter?"}, an experiment is to be conducted to test the performance of both systems. As the hypothesis is that Kafka-based solution will outperform \ac{ART} only when its performance capabilities are leveraged, the experiment is designed in a way to take into account different configurations of the Kafka replication pipeline.

\section{Experiment Design}
\label{ch05:methodology:design}
The experiment involves generating a large volume of create or update changes in Adabas in order to test the throughput of both systems over a period of time. Due to one system being composed of a single component, \ac{ART}, while the replication pipeline consists of at least 3 components (based on the degree of parallelization and distributedness), there were two throughput metrics chosen that provide comparable results. This involves measuring the number of \textbf{transactions per second}, as well as \textbf{messages per second}. In the case of Kafka replication, \textit{messages} refers to the number of total Kafka events being transferred in the pipeline. As explained in \ref{ch04:pipelinedevelopment:implementation:transformingtorelational}, due to each Adabas record being normalized and split into smaller records (to be added to different relational tables), there is more traffic in the pipeline than just the changed Adabas record. This makes it difficult to compare it to the way that \ac{ART} works, as \ac{ART} processes the entire XML \ac{REPTOR} message that it receives and splits the XML internally before writing it to the target database. To have a comparable metric of messages per second, it will instead be tracked by how many "table rows" (identical to the split records in Kafka) resulted in \ac{ART} before being written to the target database. The metric of transaction throughput is more straightforward, as both solutions process the same total number of transactions.

The message and transaction throughput of the Kafka connectors and \ac{ART} are measured with Zipkin. In the case of the Kafka connectors, there is already built-in support for tracing messages with Zipkin using so-called Kafka interceptors\footnote{\url{https://github.com/openzipkin-contrib/brave-kafka-interceptor}} to capture the Zipkin traces. For \ac{ART} and for transaction tracing, custom Zipkin traces were implemented using the Brave\footnote{\url{https://github.com/openzipkin/brave}} library. To measure the message throughput of the Kafka brokers, \ac{JMX} metrics were used. Unfortunately, the existing \ac{JMX} metrics do not support measuring the transaction throughput. The experiment is operating under the assumption that the Kafka brokers will not be the performance bottleneck in the replication pipeline if the brokers are configured in a way to optimize parallelization. This can be done by increasing the number of partitions in the Kafka topic, allowing the throughput to be increased through the higher degree of parallelization \cite{cerezo2021analysisparallelism}. Therefore, the performance metrics will be evaluated without taking the broker transaction throughput into account.
% can look at byte throughput of entire kafka system (not comparable to art though) to see which system is the bottleneck?
The experiment was also intentionally designed to measure the throughput of components up until the data reaches the \ac{JDBC} driver. This allows for the disregard for any latencies that result from the insert or update operations to the target database. The target relational database used for the experiment will be Postgres due to existing familiarity with it.

Due to the upsert nature of the \ac{JDBC} sink connector (see \ref{ch04:pipelinedevelopment:parallelizationconsiderations}), there is no real difference between insert and update operations for the replication pipeline. Since latencies regarding the target database operations and any delete operations are outside of the scope, the \ac{IS} operations can be used for simulating a high throughput situation. This makes it easier to prepare the Adabas file beforehand, as the data has to be inserted only once, and there is no need to delete or re-insert records for every experiment run. This helps not only with saving time between each run, but also with reproducibility, as the number of records, record content, and transactions remain the same.

There are multiple different Kafka pipeline setups which will be tested against the \ac{ART} solution, both to prove whether or not parallelization will play a decisive factor in outperforming \ac{ART}, and which pipeline setup will ensure maximum performance. The scenarios will be as follows:
\begin{description}
    \item [One task, One Partition]
    First, the performance of the replication pipeline will be tested without any parallelism involved: the source and sink connectors will be run on one Connect worker each, with only one task each. There will be a single Kafka broker with only one partition.

    \item[Three Tasks (One Worker), Three Partitions]
    This scenario will test parallelization capabilities of Kafka. There will be three source tasks and three sink tasks. There will be two workers total, one for the sink connector and one for the source connector. There will also be three brokers, so that each broker is partition leader for a single partition.

    \item[Seven Tasks (One Worker), Seven Partitions]
    This scenario will test the parallelization capabilities of Kafka to a higher degree than the previous scenario. There will be seven source tasks and seven sink tasks, run on one worker each. There will also be seven partitions, balanced among three brokers.

    \item[Seven Tasks (Two Workers), Seven Partitions]
    This scenario is similar to the previous one. In this case, however, the seven source tasks and seven sink tasks will be balanced on two Connect workers each. This scenario will provide an insight into whether having multiple workers provides additional throughput improvements.
    
    % \item[14 Source, 7 Sink Tasks (Two Workers), Seven Partitions]
    
\end{description}

In production environments, it is always recommended to enable replication of partitions on different brokers for additional fault tolerance. In the case of this experiment, no replication is enabled, as it can negatively affect Kafka's performance \cite{dobbelaerekafkavsrabbitmq}.

% talk about # of tasks per partition (or vice versa) - tasks partition wise for jdbc connector?

% what configurations of kafka will be tested?

% \section{Pipeline Configuration}
% \label{ch05:methodology:pipelineconfiguration}

\section{Adabas File}
\label{ch05:methodology:adabasfile}
One Adabas file will be used for testing the replication performance. It is called \textit{EMPLOYEES}, and contains sample employee data. This file was chosen because it has all of the existing field types described in \ref{ch02:fundamentals:adabas:forzos:datastructures}. Below is the \ac{FDT} showing all of the fields that will be replicated to the target database:
\begin{verbatim}
Field Description Table: File 1     (EMPLOYEES)                              
========================                Total Fields without SDT ... 31      
*************** T o p  of  F D T ***************                             
Lev  I Name I Leng  I Form  I    Options       I Predict Fld Name or DT / SY 
-----I------I-------I-------I----------------- I-----------------------------
  1  I  AA  I  008  I    A  I  DE UQ           I   PERSONNEL_ID              
  1  I  AB  I       I       I                  I   FULLNAME                  
  2  I  AC  I  020  I    A  I  NU              I   FIRST_NAME                
  2  I  AE  I  020  I    A  I  DE              I   NAME                      
  2  I  AD  I  020  I    A  I  NU              I   MIDDLE_NAME               
  1  I  AF  I  001  I    A  I  FI              I   MARSTAT                   
  1  I  AG  I  001  I    A  I  FI              I   SEX                       
  1  I  AH  I  004  I    P  I  DE NC           I   BIRTH                     
  1  I  A1  I       I       I                  I   FULL_ADDRESS              
  2  I  AI  I  020  I    A  I  MU NU           I   ADDRESS_LINE              
  2  I  AJ  I  020  I    A  I  DE NU           I   CITY                      
  2  I  AK  I  010  I    A  I  NU              I   POSTCODE                  
  2  I  AL  I  003  I    A  I  NU              I   COUNTRY                   
  1  I  A2  I       I       I                  I   TELEPHONE                 
  2  I  AN  I  006  I    A  I  NU              I   AREACODE                  
  2  I  AM  I  015  I    A  I  NU              I   PHONE                     
  1  I  AO  I  006  I    A  I  DE              I   DEPT                      
  1  I  AP  I  025  I    A  I  DE NU           I   JOBTITLE                  
  1  I  AQ  I       I       I  PE              I   INCOME                    
  2  I  AR  I  003  I    A  I  NU              I   CURRCODE                  
  2  I  AS  I  005  I    P  I  NU              I   SALARY                    
  2  I  AT  I  005  I    P  I  MU NU           I   BONUS                     
  1  I  A3  I       I       I                  I   LEAVE_DATA                
  2  I  AU  I  002  I    U  I                  I   LEAVE_DUE
  2  I  AV  I  002  I    U  I  NU              I   LEAVE_TAKEN               
  1  I  AW  I       I       I  PE              I   LEAVE_BOOKED              
  2  I  AX  I  008  I    U  I  NU              I   LEAVE_START               
  2  I  AY  I  008  I    U  I  NU              I   LEAVE_END                 
  1  I  AZ  I  003  I    A  I  DE MU NU        I   LANG                      
  1  I  BA  I  014  I    U  I  NU CR           I   INSERT                    
  1  I  BB  I  014  I    U  I  MU NU           I   UPDATE                    
\end{verbatim}

The \textit{Name} field refers to the Adabas file shortnames, which are used internally to identify the fields. Each shortname can have a longname mapped to it (shown in the \ac{FDT} as \textit{Predict Fld Name}). \textit{Form} shows the format of the fields: \textbf{A}lphanumeric, \textbf{P}acked decimals, and \textbf{U}npacked decimals. \textit{Lev} indicates the nested level of a field: for instance, ADDRESS\_LINE is a multiple that is nested in the FULL\_ADDRESS group. Meanwhile, the \textit{Options} field describes field attributes. The relevant options include MU and PE, which indicate whether the field is a multiple field or a periodic group.

\section{Environment Setup with Azure}
\label{ch05:methodology:environmentsetup}
% why azure was chosen?

\subsection{Ensuring Reproducibility}
- using terraform

- multiple experiment runs for each scenario (take average?)

- restarting VMs for every experiment run
% use of terraform, restarting every time with fresh kafka brokers and connectors - only metrics collection is kept running?


\subsection{Kafka-based Solution}

\subsection{Target Adapter}

\subsection{Metrics Collection}