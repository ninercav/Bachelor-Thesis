\chapter{Analysis and Discussion}
\label{ch07:discussion}
% Introduction to results - what were the experiment findings?
As demonstrated in Chapter \ref{ch06:results}, the Kafka Connect replication pipeline outperformed the Target Adapter even when no parallelization was implemented. The findings support the hypothesis outlined in Chapter \ref{ch05:methodology}, to the extent that leveraging parallelization had a positive impact on performance when compared to \ac{ART}. However, the additional predication that \ac{ART} would surpass Kafka Connect replication in single-threaded mode proved to be false. This chapter discusses the results and their implications in depth, and analyzes the effect of different factors on performance.

\section{Statistical Significance}
\label{ch07:discussion:statsig}
The confidence interval is used for determining the statistical significance of all average throughput measurements. It is also used to determine whether scenarios statistically differ from each other. The confidence level was set to 90\%, in accordance with the strategy outlined by \citeauthor{jain1991computer} \cite{jain1991computer}. The interval is typically inversely affected by the sample size and directly affected by variability and confidence level \cite{hazrausingci}. Since the sample size of five and confidence level are relatively low in the case of this experiment, any narrow intervals can most likely be attributed to low data variability. This will be regarded as true for all subsequent discussion. In the case of wide intervals, the same logic cannot be applied.

\subsection{Target Adapter}
The confidence interval for \ac{ART} is visualized in Figure \ref{fig:chapter07:discussion:artci}. The message throughput in Figure \ref{fig:chapter07:discussion:artavgmessageci} has error bars approximately between 1.91K and 1.96K, showing a relatively narrow interval. The narrow interval could be due to low variability in the measured data. The low variability is also supported by the small difference between the mean and median in Table \ref{tab:art:messagethroughput}.

Figure \ref{fig:chapter07:discussion:artavgtransci} shows the confidence interval for \ac{ART}'s total transaction throughput. It contains no visible bars, indicating that the interval is extremely narrow with only a 0.002 difference. This is supported by a very low standard error of 0.076 and no difference between the mean and median in Table \ref{tab:art:transactionthroughput}. A possible effect could be low data variability due to the throughput occurring in bursts instead of continually. Possible reasons for this behavior will be elaborated upon in \ref{ch07:discussion:performancevariability}.

\begin{figure}[htbp]
    \centering
    \subfloat[Message throughput confidence interval]{\label{fig:chapter07:discussion:artavgmessageci}
        \centering
        \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/art-message-ci.png}
    }
    \hfill
    \subfloat[Transaction throughput confidence interval]{\label{fig:chapter07:discussion:artavgtransci}
        \centering
        \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/art-transaction-ci.png}
    }
    \hfill
    \caption{Target Adapter throughput confidence interval}
    \label{fig:chapter07:discussion:artci}
\end{figure}

\subsection{Source Connector}
The source connector's confidence interval for both the message and transaction throughput is visualized in Figure \ref{fig:chapter07:discussion:sourceci}. The interval for Scenario 1 in Figure \ref{fig:chapter07:discussion:sourcemessageci} is also too narrow to be seen, with bounds between approximately 26.3K and 27.8K. This could also be the result of low data variability. The rest of the scenarios also have relatively narrow intervals, with the widest interval being between 124K and 133K for Scenario 5. All scenarios are significantly different from each other based on a visual test of the confidence intervals \cite{jain1991computer}, with Scenario 2 and Scenario 3 only marginally passing that test.

\begin{figure}[htbp]
    \centering
    \subfloat[Message throughput confidence interval]{\label{fig:chapter07:discussion:sourcemessageci}
        \centering
        \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/source-message-ci.png}
    }
    \hfill
    \subfloat[Transaction throughput confidence interval]{\label{fig:chapter07:discussion:sourcetransci}
        \centering
        \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/source-transaction-ci.png}
    }
    \hfill
    \caption{Source connector throughput confidence interval}
    \label{fig:chapter07:discussion:sourceci}
\end{figure}

The confidence interval for the transaction throughput can be seen in Figure \ref{fig:chapter07:discussion:sourcetransci}. Just as in Figure \ref{fig:chapter07:discussion:sourcemessageci}, Scenario 1 has a small enough interval to not be visible on the visualization. The rest of the intervals have intervals of approximately 2-3 transactions in size. The largest interval was recorded in Scenario 2, with a bound between 22.4 and 25.8 transactions. The measurements in all scenarios are statistically different from each other, indicating a statistically significant increase in transaction throughput with each scenario.

\subsection{Broker}
The confidence interval for the broker's average message throughput can be found in Figure \ref{fig:chapter07:discussion:brokermessageci}. Scenarios 1 and 4 have no visible intervals, indicating a possibly low variation in the measured throughput. Scenario 5 has an uncommonly large interval between approximately 105K and 131K. It is unlikely for data variability to have played a role in this case, as the difference between the mean and median for Scenario 5 is not much different than for Scenario 4 in Table \ref{tab:kafka:messagethroughputbroker}, while the boundary size differs significantly. It is possible that the sample size of all runs to calculate the average of Scenarios 4 and 5 are too small, as the broker's activity duration was less than 1.5 minutes (see Figure \ref{fig:chapter06:results:brokerallscenarios}). This could have led to discrepancies in the estimation and standard error.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/broker-message-ci.png}
    \caption{Broker message throughput confidence interval}
    \label{fig:chapter07:discussion:brokermessageci}
\end{figure}

The boundaries of Scenario 2 and Scenario 3 overlap. A t-test was performed with the resulting p = 0.0673, indicating that the difference between them is not statistically significant. The other scenarios differ from each other significantly, proving that in most cases the performance improved significantly with higher parallelization.

\subsection{Sink Connector}
Figure \ref{fig:chapter07:discussion:sinkmessageci} shows the confidence interval results for the sink connector's message throughput. The boundaries are too narrow to be visible for Scenarios 1-3, possibly due to low throughput variability. This is also supported visually by Figure \ref{fig:chapter06:results:sinkallscenarios} and by comparing the mean and median in Table \ref{tab:kafka:messagethroughputsink}. The boundaries for Scenario 4 and Scenario 5 are higher. However, they are still relatively narrow. This is unexpected for Scenario 5 especially, as it contains very high variability in Figure \ref{fig:chapter06:results:sinkallscenarios} and a very high difference between the mean and median in Table \ref{tab:kafka:messagethroughputsink}. A possible explanation could be similar to that of the broker, as the sink's activity in Scenario 5 was also less than 1.5 minutes, leading to a very small window of measurements.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{chapters/images/confidence-intervals/sink-message-ci.png}
    \caption{Sink connector message throughput confidence interval}
    \label{fig:chapter07:discussion:sinkmessageci}
\end{figure}

All of the scenarios are significantly different from each other based on visual evaluation, indicating that parallelization had a statistically significant impact on the performance in all measured scenarios.

\section{Effects of Parallelization}
Overall, the confidence intervals discussed in \ref{ch07:discussion:statsig} showed that the increase in parallelization had a statistically significant impact on the performance of all components in the Kafka replication pipeline. The source connector and the broker experienced the most significant improvement in throughput in Scenario 4, when the same number of tasks that were run in Scenario 3 were distributed on two workers in the source connector. This speaks not only to the positive impact of parallelization, but also to that of distributed processing. The increase in the broker's performance, despite no changes to its configuration, shows that the broker's performance scaled with the source connector. % add screenshots of cpu usage?

The same effect of distribution was not observed for the sink connector. As seen in Figure \ref{fig:chapter07:discussion:sinkmessageci}, the distribution of tasks to an additional worker improved the performance slightly. Nevertheless, the most significant performance increase occurred in Scenario 5, when additional workers and more than double the number of tasks were added to the sink connector. This shows that the sink connector could require a higher degree of parallel processing to achieve similar results to the source connector. However, at approximately 97K messages per second, the sink connector is still below the source connector's performance in Scenario 4 and Scenario 5. This leads to the possibility that the Postgres database is the actual bottleneck, and that latencies related to the upsert operations influence the performance of individual tasks. This would explain why increasing the number of tasks increases the performance more drastically than increasing the number of workers: the more tasks, the more operations can be performed despite connector or database latencies. It would be interesting for further research to investigate whether different database configurations or even different relational databases would affect the sink connector's performance. More testing would also be required to determine the optimal degree of parallelization for the source connector.

Scenario 5 proved to have an unexpected result for the source connector (and, by extension, the broker). It was expected that such a significant increase in tasks and workers would result in a more significant improvement in performance. A possible reason for that could be that the EntireX Broker queue becomes the bottleneck and cannot handle the high number of parallel tasks. A possible solution for that could be to configure multiple queues, each of which is responsible for a certain range of \ac{ISN}s. This would allow for the performance of the EntireX Broker to scale together with the performance of the source connector. It would be interesting to run more tests in the future to determine the load limit of a single EntireX Broker queue and the effect of multiple queues on overall performance.

\section{Scalability and Architectural Considerations}
% very scalable, however increased complexity, need to manage cluster and its health
% mention in prev section that performance bottleneck may exist - then clarify here that scalable up to a certain extend, what else can be done to improve that scalability (multiple queues, more jdbc tasks, etc.)
As the various scenarios have shown, the Kafka replication pipeline is extremely scalable, allowing the degree of parallelization to be increased with ease. However, as discussed in the previous section, there is the possibility of other bottlenecks in the system, such as the EntireX Broker queue and the Postgres database. The EntireX Broker can be scaled, as discussed, by adding additional queues for processing specific \ac{ISN} ranges. With regards to Postgres, the results of Scenario 5 showed that a large number of tasks had a more positive impact on the sink connector's performance. Database query and insertion latency could be the culprit, in which case scaling the sink connector to a greater degree than the source connector could be of benefit. If, however, further scaling of the sink connector does not improve performance, it might be helpful to consider running Postgres as a distributed database. This would allow for Postgres to be scaled along with the Kafka replication and with the EntireX Broker, improving not only scalability but also availability of the database. A possible strategy for scaling Postgres is with Citus, a database engine that allows for Postgres to be run and scaled as a distributed database \cite{cubukcu2021citus}.

Overall the scalability of the Kafka replication solution is a major advantage compared to \ac{ART}, as it can improve overall performance and guarantee availability. However, as with any distributed system, there is also its complexity to consider. While the performance increase might be beneficial, there is also the maintenance of the Kafka cluster to consider in a production environment. This requires constant monitoring of the cluster's health, especially in case nodes become unavailable or any unknown errors cause tasks to fail and crash. This requires additional resources, not to mention the cost for hosting such a cluster around the clock. It is therefore important to consider whether the additional cost and maintenance effort are worth the required increase in performance and scalability. The Kafka-based solution might be especially attractive for companies with an existing Kafka cluster, since they already have an existing infrastructure and trained specialists to monitor and maintain the entire replication pipeline.

\subsection{The Consistency vs. Performance Trade-off}

\section{Performance Variability}
\label{ch07:discussion:performancevariability}
% variability - different number of transactions/Adabas records per EntireX broker message - partial cause?
% dockerization, garbage collection - drops in performance
% entirex broker and postgres bottleneck?

\section{Experiment Limitations}
% only initial state tested for "consistent" results and similarity - interesting how updates would affect, as art doesn't receive full data image
% interesting in future to test flat fields only or more periodic fields
